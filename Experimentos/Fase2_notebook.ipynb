{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f857a5-7764-4f2d-b1f7-d342178d9f0d",
   "metadata": {},
   "source": [
    "# Importar librerías y definir funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8468bba-8a8a-4386-9f5b-d9fb82a7ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a3490-100d-41fa-b7ea-b002fd2ac5d7",
   "metadata": {},
   "source": [
    "# 1. Carga y limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb7d1a0-c42f-4eec-89bc-369d5204113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Insumos/consolidated_data.csv', low_memory=False)\n",
    "\n",
    "# Normalizar nombres\n",
    "date_col   = [c for c in df.columns if 'fecha' in c.lower() or 'date' in c.lower()][0]\n",
    "client_col = [c for c in df.columns if 'cliente' in c.lower() or 'client' in c.lower()][0]\n",
    "pres_col   = [c for c in df.columns if 'pres' in c.lower()][0]\n",
    "temp_col   = [c for c in df.columns if 'temp' in c.lower()][0]\n",
    "vol_col    = [c for c in df.columns if 'vol' in c.lower() and 'volu' in c.lower()][0]\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "for col in (pres_col, temp_col, vol_col):\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    date_col: 'Fecha',\n",
    "    client_col: 'Cliente',\n",
    "    pres_col: 'Presión',\n",
    "    temp_col: 'Temperatura',\n",
    "    vol_col: 'Volumen'\n",
    "})\n",
    "\n",
    "# Eliminar duplicados y imputar\n",
    "df = df.sort_values(['Cliente','Fecha']).drop_duplicates(['Cliente','Fecha'])\n",
    "frames = []\n",
    "for client, grp in df.groupby('Cliente'):\n",
    "    g = grp.set_index('Fecha').sort_index()\n",
    "    for var in ['Presión','Temperatura','Volumen']:\n",
    "        g[var] = g[var].interpolate(method='time')\n",
    "    frames.append(g.reset_index())\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Variables temporales y estandarizar\n",
    "df['hour']      = df['Fecha'].dt.hour\n",
    "df['dayofweek'] = df['Fecha'].dt.dayofweek\n",
    "df['month']     = df['Fecha'].dt.month\n",
    "features = ['Presión','Temperatura','Volumen','hour','dayofweek','month']\n",
    "df[features] = StandardScaler().fit_transform(df[features])\n",
    "\n",
    "# —————————————————————————————————————————————\n",
    "# 2. Segmentación de clientes\n",
    "# —————————————————————————————————————————————\n",
    "stats = df.groupby('Cliente')[['Presión','Temperatura','Volumen']].agg(['mean','std'])\n",
    "stats.columns = ['_'.join(c) for c in stats.columns]\n",
    "stats_scaled = StandardScaler().fit_transform(stats)\n",
    "stats['segment'] = KMeans(n_clusters=2, random_state=42).fit_predict(stats_scaled)\n",
    "#df = df.merge(stats['segment'], left_on='Cliente', right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23743e06-f4c1-49f1-b465-2517e10c7c6f",
   "metadata": {},
   "source": [
    "# 2. Segmentación de clientes con KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59817f49-3f3f-4673-b98d-801405215adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si esta disponible\n",
      "7949/7949 [==============================] - 11s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# —————————————————————————————————————————————\n",
    "# 3. Evaluación por segmento con modelo temporal\n",
    "# —————————————————————————————————————————————\n",
    "results = []\n",
    "lags = [1,2,3]  # usar 3 lags para capturar dependencia temporal\n",
    "\n",
    "for seg in range(1):\n",
    "    df_seg = df.sort_values('Fecha')\n",
    "    # crear lags\n",
    "    for lag in lags:\n",
    "        for var in ['Presión','Temperatura','Volumen']:\n",
    "            df_seg[f'{var}_lag{lag}'] = df_seg.groupby('Cliente')[var].shift(lag)\n",
    "    df_seg = df_seg.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # split 70/30\n",
    "    idx = int(len(df_seg)*0.7)\n",
    "    train = df_seg.iloc[:idx].copy()\n",
    "    test  = df_seg.iloc[idx:].copy()\n",
    "    \"\"\"\n",
    "    # simula anomalías 1% en test\n",
    "    test['anomaly'] = 0\n",
    "    n_anom = max(1,int(0.01*len(test)))\n",
    "    ani = np.random.choice(test.index, n_anom, replace=False)\n",
    "    for var in ['Presión','Temperatura','Volumen']:\n",
    "        test.loc[ani, var] *= 3\n",
    "    test.loc[ani,'anomaly'] = 1\n",
    "    \"\"\"    \n",
    "\n",
    "    # Número de anomalías\n",
    "    n_anom = max(1, int(0.01 * len(test)))\n",
    "    ani = test.sample(n=n_anom, random_state=42).index\n",
    "\n",
    "    # Etiquetar\n",
    "    test['anomaly'] = 0\n",
    "    test.loc[ani, 'anomaly'] = 1\n",
    "\n",
    "    # Para cada variable, sumar ruido de gran escala\n",
    "    for var in ['Presión','Temperatura','Volumen']:\n",
    "        sigma = test[var].std()\n",
    "        noise = np.random.normal(loc=0, scale=3*sigma, size=n_anom)\n",
    "        test.loc[ani, var] += noise\n",
    "    \n",
    "    # A) Isolation Forest\n",
    "    if_model = IsolationForest(contamination=0.015, random_state=42)\n",
    "    if_model.fit(train[features])\n",
    "    pred_if = if_model.predict(test[features])\n",
    "    pred_if = np.where(pred_if==1, 0, 1)\n",
    "    \n",
    "    # B) Modelo temporal (MLPRegressor) con lags\n",
    "    lag_features = [f'{v}_lag{l}' for l in lags for v in ['Presión','Temperatura','Volumen']]\n",
    "    X_tr = train[lag_features]\n",
    "    y_tr = train[['Presión','Temperatura','Volumen']]\n",
    "    X_te = test[lag_features]\n",
    "    \n",
    "    temp_model = MLPRegressor(hidden_layer_sizes=(len(X_tr.columns)//2,), max_iter=200, random_state=42)\n",
    "    temp_model.fit(X_tr, y_tr)\n",
    "    preds = temp_model.predict(X_te)\n",
    "    mse = np.mean((test[['Presión','Temperatura','Volumen']].values - preds)**2, axis=1)\n",
    "    thresh = np.percentile(mse, 99)\n",
    "    pred_temp = (mse > thresh).astype(int)\n",
    "\n",
    "   \n",
    "    # --------- Modelo 2: Autoencoder (reconstrucción) ---------\n",
    "    ae = MLPRegressor(hidden_layer_sizes=(len(features)//2,), activation='relu',\n",
    "                      max_iter=200, random_state=42)\n",
    "    ae.fit(train[features], train[features])\n",
    "    recon = ae.predict(test[features])\n",
    "    mse_ae = np.mean((test[features] - recon)**2, axis=1)\n",
    "    thresh_ae = np.percentile(mse_ae, 99)\n",
    "    pred_ae = (mse_ae > thresh_ae).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Modelo 5: LSTM \n",
    "\n",
    "    print(\"Si esta disponible\")\n",
    "    X_tr = train[lag_features].values.reshape(len(train), len(lags), 3)\n",
    "    y_tr = train[['Presión','Temperatura','Volumen']].values\n",
    "    X_te = test[lag_features].values.reshape(len(test), len(lags), 3)\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(50, input_shape=(len(lags),3)),\n",
    "        Dense(3)\n",
    "    ])\n",
    "    model_lstm.compile(optimizer='adam', loss='mse')\n",
    "    model_lstm.fit(X_tr, y_tr, epochs=10, batch_size=32, verbose=0)\n",
    "    preds_lstm = model_lstm.predict(X_te)\n",
    "        \n",
    "        \n",
    "    # Comparar solo las 3 variables objetivo\n",
    "    # mse_lstm = np.mean((test[features] - preds_lstm)**2, axis=1)\n",
    "    mse_lstm = np.mean((test[['Presión','Temperatura','Volumen']].values - preds_lstm)**2, axis=1)\n",
    "     \n",
    "    thresh_lstm = np.percentile(mse_lstm, 99)\n",
    "    pred_lstm = (mse_lstm > thresh_lstm).astype(int)\n",
    "\n",
    "    \n",
    "     # calcular métricas\n",
    "    metrics = {\n",
    "        'Segmento': seg,\n",
    "        'N_clients': df_seg['Cliente'].nunique(),\n",
    "        'Pre_IF': precision_score(test['anomaly'], pred_if, zero_division=0),\n",
    "        'Recall_IF':    recall_score(test['anomaly'], pred_if, zero_division=0),\n",
    "        'F1_IF':        f1_score(test['anomaly'], pred_if, zero_division=0),\n",
    "        'Pre_AE':       precision_score(test['anomaly'], pred_ae, zero_division=0),\n",
    "        'Recall_AE':          recall_score(test['anomaly'], pred_ae, zero_division=0),\n",
    "        'F1_AE':              f1_score(test['anomaly'], pred_ae, zero_division=0),\n",
    "        'Pre_Temp': precision_score(test['anomaly'], pred_temp, zero_division=0),\n",
    "        'Recall_Temp':    recall_score(test['anomaly'], pred_temp, zero_division=0),\n",
    "        'F1_Temp':        f1_score(test['anomaly'], pred_temp, zero_division=0),\n",
    "        \n",
    "               \n",
    "        'Pre_LSTM':  precision_score(test['anomaly'], pred_lstm, zero_division=0),\n",
    "        'Recall_LSTM':     recall_score(test['anomaly'], pred_lstm, zero_division=0),\n",
    "        'F1_LSTM':         f1_score(test['anomaly'], pred_lstm, zero_division=0)\n",
    "        \n",
    "    }\n",
    "    results.append(metrics)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8799d4-ec0b-4dc4-9775-a6e6ac8f6551",
   "metadata": {},
   "source": [
    "## Mostrar resultados por segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c4a99e-4236-4628-b95c-653b82ec7122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segmento</th>\n",
       "      <th>N_clients</th>\n",
       "      <th>Pre_IF</th>\n",
       "      <th>Recall_IF</th>\n",
       "      <th>F1_IF</th>\n",
       "      <th>Pre_AE</th>\n",
       "      <th>Recall_AE</th>\n",
       "      <th>F1_AE</th>\n",
       "      <th>Pre_Temp</th>\n",
       "      <th>Recall_Temp</th>\n",
       "      <th>F1_Temp</th>\n",
       "      <th>Pre_LSTM</th>\n",
       "      <th>Recall_LSTM</th>\n",
       "      <th>F1_LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.144918</td>\n",
       "      <td>0.335824</td>\n",
       "      <td>0.202466</td>\n",
       "      <td>0.670204</td>\n",
       "      <td>0.670468</td>\n",
       "      <td>0.670336</td>\n",
       "      <td>0.913129</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>0.913308</td>\n",
       "      <td>0.912343</td>\n",
       "      <td>0.912702</td>\n",
       "      <td>0.912522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Segmento  N_clients    Pre_IF  Recall_IF     F1_IF    Pre_AE  Recall_AE  \\\n",
       "0         0         20  0.144918   0.335824  0.202466  0.670204   0.670468   \n",
       "\n",
       "      F1_AE  Pre_Temp  Recall_Temp   F1_Temp  Pre_LSTM  Recall_LSTM   F1_LSTM  \n",
       "0  0.670336  0.913129     0.913488  0.913308  0.912343     0.912702  0.912522  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrar resultados por segmento\n",
    "metrics_df = pd.DataFrame(results)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede8088-ad6b-4924-8ed4-007bd8640232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
